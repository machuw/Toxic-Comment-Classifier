<!doctype html><html><head><meta charset="utf-8">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/2.10.0/github-markdown.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js">
<link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
<link rel="stylesheet" href="https://gitcdn.xyz/repo/goessner/mdmath/master/css/texmath.css">
<link rel="stylesheet" href="https://gitcdn.xyz/repo/goessner/mdmath/master/css/vscode-texmath.css">

</head><body class="markdown-body">
<h1 id="e69cbae599a8e5ada6e4b9a0e7bab3e7b1b3e5ada6e4bd8d-4" data-line="0" class="code-line">机器学习纳米学位</h1>
<h2 id="e6af95e4b89ae9a1b9e79bae-4" data-line="1" class="code-line">毕业项目</h2>
<p data-line="2" class="code-line">马远超
2019-07-01</p>
<h2 id="i-e997aee9a298e5ae9ae4b989-4" data-line="5" class="code-line">I. 问题定义</h2>
<h3 id="e9a1b9e79baee6a682e8bfb0-4" data-line="6" class="code-line">项目概述</h3>
<p data-line="7" class="code-line">互联网的本质是为了降低人们获取信息的成本，更便捷的进行沟通和分享。因此，从互联网创建时，就允许世界各地的人们通过互联网进行自由的交流、讨论以及合作。而像国内的贴吧，微博，微信，国外的Twitter，Facebook，Wikipedia等社区平台的建立，形成了这些互动可以发生的基础。为了人们在社区中可以更有序的交流以及促进对话，许多的社区都制定了自己的标准和规则，并防止这些社区被有毒行为劫持或摧毁。然而，随着有毒评论的黑色产业化，利益驱使人们通过各种手段来规避规范和标准，使得通过人为来执行这些规范和标准变得越来越困难。事实上Facebook正在招聘越来越多的版主来筛选可疑的内容[1]。同时，许多新闻网站现在也已经开始禁用评论功能[2]。而这些人工的审核监控机制，是非常低效的做法。</p>
<p data-line="9" class="code-line">综上，我们需要一种工具来自动化地对用户评论进行监视，分类和标记。此外，不同的网站可能需要监控不同类型的内容。因此需要建立一个能够区分不同类型的言语攻击行为的模型。</p>
<p data-line="11" class="code-line">我们可以看到在论文[3]中，研究人员对情感分析进行了大量研究。他们的工作重点是情绪分析，这与我们正在研究的领域非常相似。论文中定义了一种使用词袋技术预处理文本的合理方法。他们接着使用SVM和朴素贝叶斯分类器来确定推文的情绪是积极的，中性的还是负面的，并且发现朴素贝叶斯分类器更准确。此外，当他们对推文进行矢量化时，他们通过使用bigrams来提高分类器的准确性。他们的工作可以为我的benchmark model参考。</p>
<h3 id="e997aee9a298e99988e8bfb0-4" data-line="13" class="code-line">问题陈述</h3>
<p data-line="14" class="code-line">Toxic Comment Classification Challenge是kaggle上由Jigsaw提出的一个比赛，比赛中提供了带有多标签分类的Wikipedia评论数据，我们通过使用这份数据训练一个<strong>文本多类型分类器</strong>，对任意未知文本进行多标签类型（威胁，色情，侮辱和种族歧视言论等）的分类，并给出文本分别属于每个分类的概率。<strong>这是一个文本多分类问题，并属于有监督学习</strong>。</p>
<h3 id="e8af84e4bbb7e68c87e6a087-4" data-line="16" class="code-line">评价指标</h3>
<p data-line="17" class="code-line">我使用列平均的ROC AUC作为我的评估指标，它是单个类别预测结果ROC AUC的平均值。ROC曲线是在不同分类阈值下使用TPR和FRP绘制的图，而AUC则是ROC曲线下面积，当AUC值越大，当前的分类算法越有可能将正样本排在负样本前面，即能够更好的分类[7]。</p>
<p data-line="19" class="code-line">ROC空间将假阳性率（FPR）定义为X轴，真阳性率（TPR）定义为Y轴[8]。</p>
<ul>
<li data-line="20" class="code-line">TPR：真阳性率，在所有实际为阳性的样本中，被正确地判断为阳性之比率。</li>
</ul>
<section><eqn><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>T</mi><mi>P</mi><mi>R</mi><mo>=</mo><mfrac><mrow><mi>T</mi><mi>P</mi></mrow><mrow><mo>(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi><mo>)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">TPR=\frac{TP}{(TP+FN)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.29633em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.36033em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></eqn></section><ul>
<li data-line="22" class="code-line">FPR：假阳性率在所有实际为阴性的样本中，被错误地判断为阳性之比率。</li>
</ul>
<section><eqn><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>F</mi><mi>P</mi><mi>R</mi><mo>=</mo><mfrac><mrow><mi>F</mi><mi>P</mi></mrow><mrow><mo>(</mo><mi>F</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi><mo>)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">FPR=\frac{FP}{(FP+TN)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.29633em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.36033em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></eqn></section><p data-line="24" class="code-line">将同一模型每个阈值的(FPR, TPR)座标都画在ROC空间里，就成为特定模型的ROC曲线。</p>
<p data-line="26" class="code-line">AUC为ROC曲线下方的面积（Area under the Curve of ROC），它表示当随机抽取一个阳性样本和一个阴性样本，分类器正确判断阳性样本的值高于阴性样本的概率（假设阈值以上是阳性，以下是阴性）。简单说来说AUC值越大的分类器，正确率越高。</p>
<p data-line="28" class="code-line">从AUC判断分类器（预测模型）优劣的标准：</p>
<ul>
<li data-line="29" class="code-line">AUC = 1，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。</li>
<li data-line="30" class="code-line">0.5 &lt; AUC &lt; 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。</li>
<li data-line="31" class="code-line">AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。</li>
<li data-line="32" class="code-line">AUC &lt; 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。</li>
</ul>
<p data-line="34" class="code-line">同时，偏差，方差，精度，召回和F1分数也将用作评估指标，以检查过度拟合和欠拟合。</p>
<h2 id="ii-e58886e69e90-4" data-line="36" class="code-line">II. 分析</h2>
<h3 id="e695b0e68daee68ea2e7b4a2-4" data-line="37" class="code-line">数据探索</h3>
<p data-line="38" class="code-line">训练数据由Toxic Comment Classification Challenge比赛提供。数据为对恶性行为人工标注的Wikipedia评论数据，每个样本有可能被同时标注为多个类型，当所有类型的标注都为0时，表示该文本不是恶毒评论。标注的类型包括：</p>
<ul>
<li data-line="39" class="code-line">toxic</li>
<li data-line="40" class="code-line">severe_toxic</li>
<li data-line="41" class="code-line">obscene</li>
<li data-line="42" class="code-line">threat</li>
<li data-line="43" class="code-line">insult</li>
<li data-line="44" class="code-line">identity_hate</li>
</ul>
<p data-line="46" class="code-line">比赛提供的数据由如下四个文件构成：</p>
<ul>
<li data-line="47" class="code-line">train.csv - 训练集，包括159571条已进行标注的评论数据</li>
<li data-line="48" class="code-line">test.csv - 测试集，包括153164条待检测数据</li>
</ul>
<p data-line="50" class="code-line">csv文件的数据格式为：</p>
<ul>
<li data-line="51" class="code-line">id</li>
<li data-line="52" class="code-line">comment_text</li>
<li data-line="53" class="code-line">toxic</li>
<li data-line="54" class="code-line">severe_toxic</li>
<li data-line="55" class="code-line">obscene</li>
<li data-line="56" class="code-line">threat</li>
<li data-line="57" class="code-line">insult</li>
<li data-line="58" class="code-line">identity_hate</li>
</ul>
<p data-line="60" class="code-line">其中，comment_text是模型的输入。toxic，severe_toxic，obscene，threat，insult，identity_hate，如之前所诉为样本的分类标签，样本有可能同时属于多个分类。模型的输出是输入文本被<strong>分别</strong>判断为每个分类（toxic，insult等）的概率。</p>
<p data-line="62" class="code-line">同时，在训练集中，评论人工标注类型标签的个数分布如下图[4]，由图可见该数据集是一个非平衡的数据集。
<img src="/Users/mayuanchao/workspace/MLND/Toxic-Comment-Classifier/pics/Exploration-1.png" alt="" class="loading" id="image-hash-f56c077890c728220edd1f1b5b43f82f3ae34710b774ea05540e99f898e913ef"></p>
<p data-line="65" class="code-line">下图显示了训练集中每条评论的单词数。正如我们所看到的，大多数评论都有0到100个单词。去重后单词数小于或等于100的评论占比为97%。
<img src="vscode-resource:/Users/mayuanchao/workspace/MLND/Toxic-Comment-Classifier/pics/Exploration-2.png" alt="" class="loading" id="image-hash-d4b23afde6f287697ef6d31f5482cfe4d606e4b23fe8adf17548d65e33a265cf"></p>
<h3 id="e68ea2e7b4a2e680a7e58fafe8a786e58c96-4" data-line="68" class="code-line">探索性可视化</h3>
<h3 id="e7ae97e6b395e5928ce68a80e69caf-4" data-line="70" class="code-line">算法和技术</h3>
<p data-line="71" class="code-line">这个问题可以通过使用一种算法来解决，该算法将评论作为输入并输出一个概率列表，无论它是否有毒和毒性类型。文本分类最常用和简单的基线模型是朴素贝叶斯分类器和支持向量机。</p>
<p data-line="73" class="code-line"><strong>朴素贝叶斯分类器</strong>是一个基于贝叶斯规则的线性分类器：</p>
<section><eqn><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><msub><mi>w</mi><mi>j</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>X</mi><mi>i</mi></msub><mo>)</mo><mo>=</mo><mfrac><mrow><mi>P</mi><mo>(</mo><msub><mi>X</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>w</mi><mi>j</mi></msub><mo>)</mo><mo>⋅</mo><mi>P</mi><mo>(</mo><msub><mi>w</mi><mi>j</mi></msub><mo>)</mo></mrow><mrow><mi>P</mi><mo>(</mo><msub><mi>X</mi><mi>i</mi></msub><mo>)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">P(w_j|X_i) = \frac{P(X_i|w_j) \cdot P(w_j)}{P(X_i)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></eqn></section><p data-line="77" class="code-line">后验概率<eq><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><msub><mi>w</mi><mi>j</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>X</mi><mi>i</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">P(w_j|X_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></eq>可以被描述为输入属于类的概率，给定输入的特征和特征的条件概率（如果它们属于特定类）。朴素贝叶斯分类器还假设每个特征独立于其他特征。</p>
<p data-line="79" class="code-line">它们通过找出单词<eq><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span></span></span></span></eq>属于某个类的概率来识别给定一组单词<eq><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><msub><mi>w</mi><mn>1</mn></msub><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><msub><mi>w</mi><mi>n</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">(w_1..w_n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">.</span><span class="mord">.</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></eq>的类<eq><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><msub><mi>w</mi><mi>j</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>X</mi><mi>i</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">P(w_j|X_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></eq>的概率<eq><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span></span></span></span></eq>对于数据集<eq><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><msub><mi>X</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>w</mi><mi>j</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">P(X_i|w_j)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></eq>中的每个单词，并将其乘以类的概率<eq><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><msub><mi>w</mi><mi>j</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">P(w_j)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></eq>并将其除以单词出现的概率<eq><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><msub><mi>X</mi><mi>i</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">P(X_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></eq>然后最大化这种后验概率。</p>
<p data-line="81" class="code-line"><strong>支持向量机</strong>是一种监督学习模型，可用于分类和回归。 SVM将每个输入样本映射为空间中的点，并构造超平面或超平面集，以便每个类的样本除以明确的间隙。然后将新样本映射到空间中并基于平面之间的间隙预测属于特定类。</p>
<p data-line="83" class="code-line"><img src="vscode-resource:/Users/mayuanchao/workspace/MLND/Toxic-Comment-Classifier/pics/494px-SVM_margin.png" alt="" class="loading" id="image-hash-3d43e0213b3bc8059eab96f4f1aa071c121f0cdd5a51459b7c89607b1aee678e"></p>
<p data-line="85" class="code-line">结合NB和SVM分类器并且工作得相当好的算法是NBSVM[10]，它将用作基线模型。</p>
<p data-line="87" class="code-line">为了解决这个问题，我们也可以使用神经网络。神经网络是一层互连的节点，其中包含类似于人类大脑神经元的激活功能。神经网络通过输入层接收输入作为数字，并将输入传递给处理它的隐藏层。隐藏层可以是多个层。每层中的每个节点都有一个权重，节点将输入与权重相乘以得到输出。然后输出层提供单个数字，该数字基于激活函数提供概率或类。通过为每个节点分配随机权重来训练网络，并且网络自动调整权重以使预测接近实际输出。</p>
<p data-line="89" class="code-line"><img src="vscode-resource:/Users/mayuanchao/workspace/MLND/Toxic-Comment-Classifier/pics/220px-Neural_network_example.svg.png" alt="" class="loading" id="image-hash-5d651519663d98000618d268e802c8445324964c9b479caabc4dcb33443752b1"></p>
<p data-line="91" class="code-line">在我们的例子中，网络的输入是转换成数字形式的文本，其中每个单词输入到节点，输出层包含6个节点，表示6个输出类型的概率。</p>
<p data-line="93" class="code-line">然而，这些模型和其他反馈模型（如CNN）的问题是它们不跟踪顺序数据，即它们不跟踪句子中单词的上下文，并且它们对于长文本表现不佳和它们倾向于过度训练数据。</p>
<p data-line="95" class="code-line">为了克服这个问题，我使用了递归神经网络（RNN）。在递归神经网络中，每个节点具有反馈回路，该回路在给定时间步长处获取先前输入。这允许RNN展示时间序列的时间动态行为。 RNN具有短期存储器，允许其基于节点的权重和先前的输入决策来确定当前输入。如下所示，反馈环路使用前n个输入序列的输入处理输入。</p>
<p data-line="97" class="code-line"><img src="vscode-resource:/Users/mayuanchao/workspace/MLND/Toxic-Comment-Classifier/pics/640px-Recurrent_neural_network_unfold.svg.png" alt="" class="loading" id="image-hash-5b8da2ffd649f5d304f4e9bd60b8e0fd266823106eac8fa36be6007c69688c92"></p>
<p data-line="99" class="code-line">长短期记忆网络[11]是一种回归神经网络算法，是一种专为自然语言处理而设计的算法，经证明可以很好地运行，并且可以作为解决方案的基础。LSTM网络克服了传统RNN无法从长时间运行的顺序数据中获取上下文信息的缺点，它可以记住长时间的重要信息。</p>
<p data-line="101" class="code-line">LSTM单元如下所示。每个单元有3个门：输入门，输出门和遗忘门。门由S形和矢量运算表示。输入门决定需要在单元状态中存储哪些新信息。遗忘门如果不重要则删除旧信息，输出门使输入影响当前时间步的输出。每个门都是一个输出在0和1之间的sigmoid层，其中0表示不通过，1表示让一切通过。</p>
<p data-line="103" class="code-line"><img src="vscode-resource:/Users/mayuanchao/workspace/MLND/Toxic-Comment-Classifier/pics/LSTM3-chain.png" alt="" class="loading" id="image-hash-9e68a0206dc4cd6340688875387bd2d70ddf299953a4d766930144e9012a583c"></p>
<p data-line="105" class="code-line">LSTM单元有3个输入：输入样本<eq><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span></span></span></span></eq>在<eq><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.61508em;vertical-align:0em;"></span><span class="mord mathdefault">t</span></span></span></span></eq>时刻<eq><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">(x_t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></eq>，前一个LSTM单元格的记忆<eq><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><msub><mi>C</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">(C_{t-1})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></eq>，前一个LSTM单元格的输出<eq><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">(h_{t-1})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></eq>，</p>
<p data-line="107" class="code-line">LSTM单元完成的步骤：</p>
<ol>
<li data-line="108" class="code-line">前一个单元格的输出和当前输入通过遗忘层忘记旧信息。</li>
<li data-line="109" class="code-line">下一层节点决定需要在单元状态中存储哪些新信息。这是通过识别要更新的值并创建新值的向量来完成的。</li>
<li data-line="110" class="code-line">接下来，通过将单元状态与步骤1（遗忘向量）和步骤2（新值）的输出相乘，来将旧单元得状态更新为新单元状态。</li>
<li data-line="111" class="code-line">然后，将输入和新记忆相乘以得到输出。</li>
</ol>
<p data-line="113" class="code-line">有一种LSTM的形式称为双向LSTM，其具有两个网络，一个用于正常文本序列，一个用于反向序列文本。此外，在LSTM层之后使用卷积神经网络层，其从网络中提取局部特征。</p>
<p data-line="115" class="code-line">在将数据输入到网络之前，需要对文本数据进行预处理（分词等）。另外，我们还需要将文本数据转换为数字形式，其中每个单词由数字表示。然后，该数值数据用于将每个评论转换为具有向量表示（one-hot编码）。然而，one-hot编码这种向量表现形式是稀疏且低效的。为了克服这一点，常将每个单词表示为词嵌入向量，并且，具有相似含义的单词的词嵌入向量将具有相似的表示。</p>
<p data-line="117" class="code-line"><strong>词嵌入</strong></p>
<p data-line="119" class="code-line">要将文本数据提供给神经网络模型训练，我们需要将它们标记为数字的形式，最常见形式为one-hot编码。但是它用来表示词向量时候，非常的稀疏且低效。
为了克服这个问题，向量空间模型被创建出来。在向量空间中，每个单词可以用一个稠密的向量来表示，且空间中距离相近的单词会有相似的语义。 向量空间模型依赖于分布假设，该假设指出出现在相同上下文中的单词具有相似含义。
遵循这一原则的两种方法是：</p>
<ol>
<li data-line="122" class="code-line">基于频率的嵌入 - 查找数据集中每个单词的频率，为每个单词创建密集向量。</li>
<li data-line="123" class="code-line">基于预测的嵌入 - 使用神经网络，根据给定词汇表中每个单词的上下文创建词嵌入。</li>
</ol>
<p data-line="125" class="code-line">我正在进行基于预测的嵌入，因为它更加密集和高效，并且有预先训练好的字嵌入，例如Google和stanford的GloVe<sup>[12]</sup>和Facebook的FastText <sup>[13][14][15]</sup>。</p>
<p data-line="127" class="code-line">具体来说，我选择了FastText作为预训练的单词嵌入，因为它的表现平均比GloVe更好（尽管不是很大）。Fasttext基于word2vec，其中每个单词使用n-gram分成子单词，最终的嵌入向量将是所有n-gram的向量之和。</p>
<p data-line="129" class="code-line">FastText的词嵌入模型使用的是skip-gram，并使用Common Crawl数据集进行模型的训练。该数据集包含6000亿个令牌并包含200万个单词向量。</p>
<p data-line="131" class="code-line">完成嵌入模型后，定义模型并将数据拟合到模型中，并使用测试数据测试模型。</p>
<p data-line="133" class="code-line">总结一下：</p>
<ol>
<li data-line="134" class="code-line">数据探索。会对训练数据集中的平均分类进行分布统计，并创建可视化图形。此外，还可以创建词云图，以了解每个类别中的常用词。同时，了解数据集中的独特单词，常出现单词，填充单词等，对于数据集的理解也很重要。</li>
<li data-line="135" class="code-line">预处理数据。例如空值处理，异常处理处理等。在预处理期间，需要删除所有不需要的数据。这包括可能包含随机字母或单词的垃圾数据，非文本数据，用户名等。预处理措施一般包括：
<ul>
<li data-line="136" class="code-line">大写变小写</li>
<li data-line="137" class="code-line">去掉停顿词，标点，空白文本，英文之外的其他文本</li>
<li data-line="138" class="code-line">分词</li>
<li data-line="139" class="code-line">词性标注 - 帮助我们更好的理解单词/句子的含义</li>
<li data-line="140" class="code-line">词干提取 - 减少输入的语料库</li>
<li data-line="141" class="code-line">生成文档矩阵后计算tf-idf，去除频率较低的单词（例如去掉频率小于5的，或去掉在60%文档中出现的单词）</li>
</ul>
</li>
<li data-line="142" class="code-line">使用词嵌入创建数据的向量表示</li>
<li data-line="143" class="code-line">定义模型并定义模型的各个层。我定义的模型以词嵌入向量作为输入，接着是LSTM层和CNN层。输出层使用sigmoid作为激活函数。</li>
<li data-line="144" class="code-line">使用AUROC作为度量标准来训练和测试模型。</li>
</ol>
<h3 id="e59fbae58786e6a8a1e59e8b-4" data-line="146" class="code-line">基准模型</h3>
<p data-line="147" class="code-line">SVM是最常用的文本分类算法之一，可用作基准模型。基于SVM和朴素贝叶斯算法的SVMNB[6]，它提供了比传统SVM更好的性能，是在kaggle比赛中的推荐的benchmark，我将使用SVMNB作为我的benchmark model。</p>
<h2 id="iii-e696b9e6b395-4" data-line="149" class="code-line">III. 方法</h2>
<h3 id="e695b0e68daee9a284e5a484e79086-4" data-line="150" class="code-line">数据预处理</h3>
<p data-line="151" class="code-line">Jigsaw提供的维基媒体评论数据大多是干净的，没有任何空值。但正如上一节所述，数据存在类不平衡问题。大多数数据都很干净，甚至在有毒数据中也有一些类别。但是，选择的度量标准AUROC不受此类不平衡问题的影响。</p>
<p data-line="153" class="code-line">在数据集上完成的预处理步骤是：</p>
<ol>
<li data-line="154" class="code-line">识别并处理数据集中的所有空值 - 数据集不会包含任何空值。</li>
<li data-line="155" class="code-line">删除数据集中的特殊字符和数字 - 它们没有贡献模型的任何内容，使用简单的正则表达式删除。这也需要处理表情符号，IP地址等。</li>
<li data-line="156" class="code-line">将所有文本转换为小写并将每个评论拆分为单词组成的数组。</li>
<li data-line="157" class="code-line">从文本中删除停顿词 - 停顿词是像a，to等词。这些单词不提供任何上下文信息，不需要添加到模型中。</li>
<li data-line="158" class="code-line">使用提取的词干替代单词 - 词干提取，即删除任何前缀和后缀，减少了词汇空间。</li>
<li data-line="159" class="code-line">评论被修剪和填充到100个单词的统一长度 - 每个评论可能具有不同的长度，需要将其归一化。因为神经网络期望每个评论具有相同数量的特征。这里长度选择为100，是因为如数据探索部分（图2）所示，大多数注释包含少于100个单词。</li>
</ol>
<p data-line="161" class="code-line">这些预处理步骤显著减小了输入的大小。</p>
<p data-line="163" class="code-line">完成此操作后，需要将文本数据编码为数值。使用词嵌入稠密的向量表示。如前所述，选择FastText预训练嵌入作为嵌入层。</p>
<h3 id="e689a7e8a18ce8bf87e7a88b-4" data-line="164" class="code-line">执行过程</h3>
<p data-line="165" class="code-line">模型描述：</p>
<ol>
<li data-line="166" class="code-line">输入层接受长度为100的单词索引列表（如在上一节中所述，每个注释都被转换为长度为100的向量）。</li>
</ol>
<pre data-line="168" class="code-line"><code>input = Input(shape=(maxlen,)) # maxlen=100
</code></pre>
<ol start="2">
<li data-line="170" class="code-line">网络的第一层是嵌入层。嵌入层接收包含整数序列作为输入的字向量，并基于权重将这些整数转换为密集向量。权重由fastText提供。该层的输出是尺寸为300的可训练特征向量（嵌入向量维数）</li>
</ol>
<pre data-line="172" class="code-line"><code>`x = Embedding(max_features, embed_size, weights=[embedding_matrix])(input)`
# max_features = 20000（词汇表的大小，即数据集中最常见的20k），embed_size = 300（fastText预训练矢量维度）
</code></pre>
<ol start="3">
<li data-line="175" class="code-line">添加丢失层用于正则化以避免过度拟合并且添加空间丢失，因为相邻矢量是相关的并且被推荐在第一层之后。空间丢失会丢弃整个特征向量（单词的嵌入表示）。</li>
</ol>
<pre data-line="177" class="code-line"><code>x = SpatialDropout1D(0.2)(x)
</code></pre>
<ol start="4">
<li data-line="179" class="code-line">下一层是LSTM层，使用双向LSTM层<sup>[16]</sup>。双向lstm层可以通过训练2个LSTM来获取过去和未来状态的信息，一个在正常输入序列上，另一个在输入序列的反向副本上。双向LSTM显示出比正常LSTM更快收敛和更好的性能<sup>[17]</sup>。网络大小根据试验和错误定义为120。</li>
</ol>
<pre data-line="181" class="code-line"><code>x = Bidirectional(LSTM(120, return_sequences=True,name='lstm_layer'))(x)
</code></pre>
<ol start="5">
<li data-line="183" class="code-line">下一层是1D CNN层，内核大小为3，显示出比简单的LSTM模型更好<sup>[18]</sup>。</li>
</ol>
<pre data-line="185" class="code-line"><code>x = Conv1D(60, kernel_size = 3, padding = &quot;valid&quot;, kernel_initializer = &quot;he_uniform&quot;)(x)
</code></pre>
<ol start="6">
<li data-line="187" class="code-line">下一层是池化层，它是全局最大池和全局平均池的组合，并将输入数据减少到1d向量。</li>
</ol>
<pre data-line="189" class="code-line"><code>avg_pool = GlobalAveragePooling1D()(x) 
max_pool = GlobalMaxPooling1D()(x)
x = concatenate([avg_pool, max_pool])
</code></pre>
<ol start="7">
<li data-line="193" class="code-line">下一层是dropout层</li>
</ol>
<pre data-line="195" class="code-line"><code>x = Dropout(0.2)(x)
</code></pre>
<ol start="8">
<li data-line="197" class="code-line">输出层有6个节点，并应用sigmoid函数来获得6个输出标签的预测。</li>
</ol>
<p data-line="200" class="code-line">我使用二元交叉熵作为损失函数来训练模型，并且在2个中批量大小为1280来优化和训练模型。使用具有1280个cuda核的Gtx 1060来训练模型。
模型实现的代码很简单，代码通过跟踪keras和scikit-learn文档来完成，其中包含用于实现模型，度量，交叉验证和嵌入的详细文档。最难的部分是建立环境，安装keras然后尝试nvidia-docker，而不是手动安装所有东西，因为一切都在本地机器（使用Gtx 1060显卡的笔记本电脑）完成。</p>
<h3 id="e5ae8ce59684-4" data-line="202" class="code-line">完善</h3>
<h2 id="iv-e7bb93e69e9c-4" data-line="204" class="code-line">IV. 结果</h2>
<h3 id="e6a8a1e59e8be79a84e8af84e4bbb7e4b88ee9aa8ce8af81-4" data-line="205" class="code-line">模型的评价与验证</h3>
<h3 id="e59088e79086e680a7e58886e69e90-4" data-line="206" class="code-line">合理性分析</h3>
<h2 id="v-e9a1b9e79baee7bb93e8aeba-4" data-line="208" class="code-line">V. 项目结论</h2>
<h3 id="e7bb93e69e9ce58fafe8a786e58c96-4" data-line="209" class="code-line">结果可视化</h3>
<h3 id="e5afb9e9a1b9e79baee79a84e6809de88083-4" data-line="210" class="code-line">对项目的思考</h3>
<h3 id="e99c80e8a681e5819ae587bae79a84e694b9e8bf9b-4" data-line="211" class="code-line">需要做出的改进</h3>
<h3 id="reference-4" data-line="214" class="code-line">Reference</h3>
<ol>
<li data-line="215" class="code-line"><a href="http://fortune.com/2018/03/22/human-moderators-facebook-youtube-twitter/">http://fortune.com/2018/03/22/human-moderators-facebook-youtube-twitter/</a></li>
<li data-line="216" class="code-line"><a href="https://www.theguardian.com/science/brain-flapping/2014/sep/12/comment-sections-toxic-moderation">https://www.theguardian.com/science/brain-flapping/2014/sep/12/comment-sections-toxic-moderation</a></li>
<li data-line="217" class="code-line"><a href="http://crowdsourcing-class.org/assignments/downloads/pak-paroubek.pdf">http://crowdsourcing-class.org/assignments/downloads/pak-paroubek.pdf</a></li>
<li data-line="218" class="code-line"><a href="https://github.com/udacity/cn-machine-learning/blob/master/toxic-comment-classification/pics/hist.png">https://github.com/udacity/cn-machine-learning/blob/master/toxic-comment-classification/pics/hist.png</a></li>
<li data-line="219" class="code-line"><a href="https://www.researchgate.net/profile/Sepp_Hochreiter/publication/13853244_Long_Short-term_Memory/links/5700e75608aea6b7746a0624/Long-Short-term-Memory.pdf">https://www.researchgate.net/profile/Sepp_Hochreiter/publication/13853244_Long_Short-term_Memory/links/5700e75608aea6b7746a0624/Long-Short-term-Memory.pdf</a></li>
<li data-line="220" class="code-line"><a href="https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf">https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf</a></li>
<li data-line="221" class="code-line"><a href="http://alexkong.net/2013/06/introduction-to-auc-and-roc/">http://alexkong.net/2013/06/introduction-to-auc-and-roc/</a></li>
<li data-line="222" class="code-line"><a href="https://zh.wikipedia.org/wiki/ROC%E6%9B%B2%E7%BA%BF">https://zh.wikipedia.org/wiki/ROC曲线</a></li>
<li data-line="223" class="code-line"><a href="https://github.com/Kirupakaran/Toxic-comments-classification/blob/master/proposal.pdf">https://github.com/Kirupakaran/Toxic-comments-classification/blob/master/proposal.pdf</a></li>
<li data-line="224" class="code-line"><a href="https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf">Baselines and Bigrams</a></li>
<li data-line="225" class="code-line"><a href="https://www.researchgate.net/publication/13853244_Long_Short-term_Memory">Long Short Term Memory</a></li>
<li data-line="226" class="code-line"><a href="https://nlp.stanford.edu/projects/glove/">gloVe</a></li>
<li data-line="227" class="code-line"><a href="https://arxiv.org/abs/1607.04606">Enriching Word Vectors with Subword Information</a></li>
<li data-line="228" class="code-line"><a href="https://arxiv.org/abs/1712.09405">Advances in Pre-Training Distributed Word Representations</a></li>
<li data-line="229" class="code-line"><a href="https://fasttext.cc/">fastText</a></li>
<li data-line="230" class="code-line"><a href="https://maxwell.ict.griffith.edu.au/spl/publications/papers/ieeesp97_schuster.pdf">Bidirectional Recurrent Neural Networks</a></li>
<li data-line="231" class="code-line"><a href="vscode-resource:/Users/mayuanchao/workspace/MLND/Toxic-Comment-Classifier/ftp:/ftp.idsia.ch/pub/juergen/nn_2005.pdf">Framewise Phoneme Classification with Bidirectional LSTM and Other Neural Network Architectures</a></li>
<li data-line="232" class="code-line"><a href="https://www.academia.edu/35947062/Twitter_Sentiment_Analysis_using_combined_LSTM-CNN_Models">Twitter Sentiment Analysis using combined LSTM-CNN Models</a></li>
</ol>

</body></html>